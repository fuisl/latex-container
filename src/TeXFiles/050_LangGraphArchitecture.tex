%=============================================================================
\chapter{LangGraph Industrial Architecture}
\label{ch:LangGraphArchitecture}
\section{Key Architectural Decisions}
\label{sec:KeyArchDecisions}
%=============================================================================

Several architectural decisions shaped the platform's design. These were
documented as lightweight Architecture Decision Records (ADRs)
\cite{nygard2011adr}.
%=============================================================================
% Page budget: 7 pages (Pages 30--36)
%=============================================================================

This chapter describes the production architecture of the AgileTest Agent
platform built with LangGraph. The architecture shifts from exploratory
prototypes toward an industrial-grade system with deterministic behavior,
human-in-the-loop control, and operational robustness.

The agent is not a monolithic component but a collection of loosely coupled
services with defined responsibilities. This separation enables independent
scaling, governance, and observability, following cloud-native design
principles \cite{googleSRE,microsoftWellArchitected}.

%=============================================================================
\section{Component Architecture}
\label{sec:ComponentArchitecture}
%=============================================================================

The LangGraph platform has five primary components: the LangGraph API service,
persistence and governance services, language model provider, observability
tooling, and external clients.

The LangGraph API service is the core execution engine. It exposes HTTP
endpoints for graph invocation, interrupt handling, and resume operations.
Graph definitions are loaded from a registry and executed as state machines
rather than ad-hoc control flows \cite{langgraphOverview}.

Persistence is split by concern. PostgreSQL stores graph threads and
checkpoints, enabling pause-and-resume across failures or restarts. Redis
handles operational state: token usage counters and per-organization
consumption limits. This follows common practice of using relational storage
for durable state and in-memory stores for operational metadata
\cite{postgresDocs,redisDocs}.

Language model access is abstracted behind a provider interface, so graph
logic is not coupled to a specific vendor. This supports model substitution
without graph redesign \cite{langchain}.

Observability uses Langfuse callbacks for traces, latency, and usage metrics.
Observability is best-effort: tracing failures must not interrupt core
execution \cite{langfuseDocs,googleSRE}.

External clients (web applications, internal services) interact only through
the API layer and never manipulate graph state directly.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{Figures/langgraph-component.png}
    \caption{Component diagram showing the LangGraph Server architecture and
        its major services.}
    \label{fig:langgraph_component}
\end{figure}

%=============================================================================
\section{Deployment Architecture (Cloud Run)}
\label{sec:DeploymentArchitecture}
%=============================================================================

The platform is deployed on Google Cloud Run, a fully managed container runtime
that supports automatic scaling and request-based billing
\cite{cloudRunOverview}. Cloud Run was selected to reduce operational overhead
while maintaining fine-grained control over service boundaries.

Two services are deployed independently. The primary service hosts the
LangGraph API and handles all request execution. A secondary cron service is
responsible for scheduled maintenance tasks, such as cleaning up Redis usage
keys. Separating these concerns avoids introducing background workloads into
the request path.

Cloud Run’s concurrency model allows multiple graph executions to be handled by
a single container instance. This makes connection pooling, particularly for
Redis, a critical consideration to avoid resource exhaustion under load.

Secrets such as API keys are injected via the platform’s secret management
mechanism rather than baked into images. Network access to persistence services
is restricted to minimize blast radius in case of compromise, aligning with
zero-trust deployment principles \cite{microsoftWellArchitected}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{Figures/langgraph-cloud.png}
    \caption{Deployment topology on Google Cloud Run showing service boundaries
        and infrastructure dependencies.}
    \label{fig:deployment_topology}
\end{figure}

Besides, Langfuse also being hosted on GCP and being protected behind a VPN as in Figure~\ref{fig:langfuse}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/langgraph-langfuse.png}
    \caption{Deployment of Observability Platform (Langfuse) on Google Cloud Platform}
    \label{fig:langfuse}
\end{figure}

%=============================================================================
\section{Runtime Request Lifecycle}
\label{sec:RuntimeLifecycle}
%=============================================================================

A request begins when a client invokes a graph through the API service. Each
request is associated with a thread identifier representing the execution
context across invocations.

When the graph reaches an interrupt point, state is checkpointed to persistent
storage and execution pauses. The API returns a response indicating that human
input is needed. On resume, execution continues from the checkpoint rather
than restarting \cite{langgraphInterrupts}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{Figures/langgraph-interrupt.png}
    \caption{Invoke, interrupt, and resume phases of graph execution.}
    \label{fig:invoke_resume_sequence}
\end{figure}

Callbacks record usage metrics and traces throughout execution. Governance
checks (token limit enforcement) are applied before model calls to prevent
uncontrolled consumption.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/langgraph-redis.png}
    \caption{Control points where validation and governance checks are applied.}
    \label{fig:control_points}
\end{figure}

%=============================================================================
\section{Key Architectural Decisions}
\label{sec:KeyArchDecisions}
%=============================================================================

Several architectural decisions were central to the platform’s design. These
decisions were documented using lightweight Architecture Decision Records
(ADRs), following established best practices \cite{nygard2011adr}.

\begin{table}[htbp]
    \centering
    \caption{Summary of key architectural decisions.}
    \label{tab:ADRSummary}
    \begin{tabular}{lp{4cm}p{5cm}}
        \toprule
        \textbf{Decision}  & \textbf{Options Considered} & \textbf{Trade-offs}                             \\
        \midrule
        State machines     & Custom logic, LangGraph     & LangGraph provides persistence and interrupts   \\
        HITL control       & Polling, Interrupts         & Interrupts give explicit pause/resume semantics \\
        Structured outputs & Free-form text, Schemas     & Schemas reduce drift and validation errors      \\
        Observability      & Strict, Best-effort         & Best-effort avoids availability impact          \\
        \bottomrule
    \end{tabular}
\end{table}

%=============================================================================
\chapter{Graph Design and Runtime Behavior}
\label{ch:GraphDesign}
%=============================================================================
% Page budget: 7 pages (Pages 37--43)
%=============================================================================

This chapter covers the internal design of LangGraph graphs used by the
AgileTest Agent. Unlike traditional workflows, these graphs explicitly model
state, transitions, and validation boundaries, enabling controlled agent
behavior over unconstrained autonomy.

%=============================================================================
\section{State and Schema Design}
\label{sec:StateSchemaDesign}
%=============================================================================

Each graph operates over a strongly typed state object that represents the
entire execution context. Reducers define how state is updated across node
transitions, ensuring that updates are explicit and auditable.

All language model outputs are constrained using structured output schemas.
This approach reduces ambiguity, simplifies validation, and enables reliable
downstream processing \cite{langchainStructuredOutput}.

Schema validation failures are treated as recoverable errors. When validation
fails, execution can retry with stricter prompts or fall back to a safe abort
path, rather than propagating invalid state.

\begin{figure}[htbp]
    \centering
    % \includegraphics[width=0.9\textwidth]{Figures/state-schema-reducers.pdf}
    \caption{State schema and reducer ownership across graph nodes.}
    \label{fig:state_schema}
\end{figure}

\begin{figure}[htbp]
    \centering
    % \includegraphics[width=0.8\textwidth]{Figures/structured-output-boundary.pdf}
    \caption{Structured output boundaries showing schema validation points.}
    \label{fig:structured_output}
\end{figure}

%=============================================================================
\section{Graph A: Requirement to Test Cases}
\label{sec:GraphA}
%=============================================================================

Graph A transforms a natural-language requirement into candidate test cases.
Execution begins in a generation state, transitions to a human review gate,
and may loop back for regeneration based on feedback.

This design models iteration as a state transition rather than embedding it in
prompt logic, making the system easier to reason about and debug
\cite{langgraphOverview}.

\begin{figure}[htbp]
    \centering
    % \includegraphics[width=\textwidth]{Figures/graph-a-state-machine.pdf}
    \caption{State machine for Graph A: Requirement $\rightarrow$ Test Cases.}
    \label{fig:graph_a_state_machine}
\end{figure}

%=============================================================================
\section{Graph B: Test Case to Test Steps}
\label{sec:GraphB}
%=============================================================================

Graph B refines a single test case into a sequence of executable steps. Beyond
generation, this graph supports explicit edit commands: add, delete, and update
operations.

Edits are validated against schema constraints before being applied, ensuring
partial updates cannot corrupt state. Rejection with feedback transitions the
graph back into a regeneration state.

\begin{figure}[htbp]
    \centering
    % \includegraphics[width=\textwidth]{Figures/graph-b-state-machine.pdf}
    \caption{State machine for Graph B: Test Case $\rightarrow$ Test Steps.}
    \label{fig:graph_b_state_machine}
\end{figure}

%=============================================================================
\section{HITL Interrupt and Resume Lifecycle}
\label{sec:HITLLifecycle}
%=============================================================================

When a graph reaches a human-in-the-loop interrupt, execution state is
checkpointed and associated with a thread identifier. Only validated state is
persisted, ensuring resumption begins from a consistent snapshot.

Upon resume, user input is validated before being merged into state. Execution
then continues deterministically from the interrupt point rather than replaying
earlier nodes.

This lifecycle enables reliable collaboration between humans and agents while
preserving strong execution guarantees
\cite{langgraphInterrupts,anthropicAgents2024}.

\begin{figure}[htbp]
    \centering
    % \includegraphics[width=\textwidth]{Figures/hitl-lifecycle-timeline.pdf}
    \caption{HITL interrupt and resume lifecycle with checkpoint storage.}
    \label{fig:hitl_timeline}
\end{figure}

