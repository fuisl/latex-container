%=============================================================================
\chapter{Local Model Serving and Evaluation}
\label{ch:LocalModelServing}
%=============================================================================
% Page budget: 4--5 pages
%=============================================================================

This chapter documents the exploration and evaluation of local large language
model (LLM) serving as part of the early research and development phase of the
internship. The objective of this work was not to deliver a production-ready
serving stack, but to understand the feasibility, trade-offs, and limitations
of running LLMs locally for rapid experimentation, cost control, and early
agent prototyping.

The findings from this track informed later architectural decisions, including
the transition toward managed and production-oriented serving solutions in
subsequent work.

%=============================================================================
\section{Objectives and Scope}
\label{sec:LocalServing_Objectives}
%=============================================================================

Local model serving was explored with three primary objectives:

\begin{itemize}
    \item To enable low-cost experimentation with LLM-based features during
          early development.
    \item To gain hands-on understanding of model inference behavior, latency,
          and resource utilization.
    \item To evaluate whether locally hosted models could reasonably support
          interactive AI-assisted workflows.
\end{itemize}

This work explicitly focused on development and experimentation scenarios.
Concerns such as multi-tenant isolation, high availability, and horizontal
scaling were considered out of scope at this stage.

%=============================================================================
\section{Model Selection}
\label{sec:LocalServing_ModelSelection}
%=============================================================================

For initial prototyping, an open-source large language model was selected to
balance development flexibility with computational feasibility. The chosen
model was \texttt{gpt-oss:20b}, an open-weight model documented by OpenAI as
providing reasoning-oriented capabilities comparable to smaller hosted
reasoning models~\cite{openai2025gptoss}.

The primary motivation for this choice was not absolute performance, but
transparency and controllability. Using an open-weight model enabled direct
inspection of inference behavior and avoided dependency on external API costs
or rate limits during early experimentation.

For retrieval-augmented generation (RAG) experiments, an open-source embedding
model, \texttt{nomic-embed-text}, was used to generate vector representations
of textual content~\cite{nomic2025embed}. This pairing allowed the full RAG
pipeline to operate locally without reliance on third-party embedding services.

%=============================================================================
\section{Serving Architecture}
\label{sec:LocalServing_Architecture}
%=============================================================================

Models were served locally using \texttt{Ollama}, a lightweight runtime designed
to simplify execution of open-source LLMs on local hardware~\cite{ollama}.
Ollama provides a uniform HTTP-based interface for inference, abstracting away
many low-level concerns related to model loading and runtime configuration.

The local serving setup followed a simple architecture:

\begin{itemize}
    \item The LLM runtime was hosted on a development machine using Ollama.
    \item Client applications issued HTTP requests to the local inference
          endpoint.
    \item Responses were integrated into downstream components such as agent
          prototypes and RAG pipelines.
\end{itemize}

This architecture minimized operational overhead and enabled rapid iteration.
However, it also tightly coupled model performance to local hardware
capabilities, which became a significant limiting factor during evaluation.

%=============================================================================
\section{Development Experience and Limitations}
\label{sec:LocalServing_Limitations}
%=============================================================================

From a development perspective, local model serving proved valuable for
experimentation and debugging. The ability to run inference without network
latency or external dependencies simplified early prototyping and allowed
frequent iteration on prompts, tool interfaces, and agent logic.

At the same time, several limitations became apparent. Inference latency was
significantly higher than that of hosted models, particularly for longer
contexts or multi-step reasoning tasks. This latency directly impacted the
usability of interactive workflows, making real-time human-in-the-loop
scenarios impractical in some cases.

Resource utilization was another constraint. Running a 20B-parameter model
locally required substantial memory and compute resources, limiting parallel
requests and reducing system responsiveness under load. These characteristics
highlighted the difficulty of scaling local serving beyond single-user or
low-concurrency scenarios.

An additional consideration was API compatibility. While Ollama provides a
convenient inference interface, it does not fully support emerging unified
response abstractions such as OpenAIâ€™s Response API. This divergence introduces
integration friction when attempting to share tooling or abstractions across
local and hosted environments.

%=============================================================================
\section{Evaluation and Outcomes}
\label{sec:LocalServing_Evaluation}
%=============================================================================

The evaluation of local model serving was primarily qualitative, based on
developer experience, responsiveness, and suitability for iterative
development. No formal benchmarking against hosted models was conducted, as
the primary goal was architectural exploration rather than performance
optimization.

The key outcome of this work was a clear understanding of the role local model
serving can play in an industrial AI development workflow. Local serving is
well suited for early-stage research, experimentation, and cost-sensitive
prototyping. However, it is poorly aligned with production requirements such
as low latency, scalability, and operational reliability.

These findings directly influenced subsequent architectural decisions,
including the move toward managed serving platforms and more structured
orchestration frameworks in later phases of the internship.

%=============================================================================
\section{Key Learning Points}
\label{sec:LocalServing_Learnings}
%=============================================================================

Several important lessons emerged from this exploration:

\begin{itemize}
    \item Local model serving lowers the barrier to experimentation but does
          not eliminate fundamental performance constraints.
    \item Open-weight models provide transparency and control, but at the cost
          of increased operational complexity.
    \item Serving architecture choices strongly influence downstream design,
          particularly for interactive and human-in-the-loop systems.
\end{itemize}

Overall, this work established a practical baseline for understanding local
LLM serving and clarified why subsequent development efforts favored
production-oriented serving and orchestration approaches.