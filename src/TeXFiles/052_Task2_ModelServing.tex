%=============================================================================
\chapter{Local Model Serving and Evaluation}
\label{ch:LocalModelServing}
%=============================================================================
% Page budget: 4--5 pages
%=============================================================================

This chapter presents the exploration and evaluation of local large language
model (LLM) serving conducted during the early and intermediate research phases
of the internship. The objective was not to build a production-ready serving
platform, but to understand the practical feasibility, performance boundaries,
and operational trade-offs of running LLMs on self-managed infrastructure.

The work progressed from lightweight local prototyping to a more structured
serving setup using vLLM, enabling realistic evaluation of latency,
throughput, and hardware utilization. The findings from this chapter directly
informed the subsequent transition toward managed, production-oriented serving
solutions.

%=============================================================================
\section{Objectives and Scope}
\label{sec:LocalServing_Objectives}
%=============================================================================

Local model serving was investigated with the following objectives:

\begin{itemize}
    \item to enable low-cost experimentation during early development without
          dependence on hosted APIs,
    \item to develop an empirical understanding of inference latency,
          throughput, and GPU resource constraints,
    \item to assess whether locally hosted models could support interactive
          and human-in-the-loop workflows under moderate concurrency.
\end{itemize}

The scope of this work was intentionally limited to single-node deployments.
Concerns such as multi-tenant isolation, high availability, autoscaling, and
global traffic management were treated as out of scope and deferred to later
phases.

%=============================================================================
\section{Model Selection}
\label{sec:LocalServing_ModelSelection}
%=============================================================================

For generation tasks, the open-weight model \texttt{gpt-oss:20b} was selected
as the primary experimental baseline. According to its model card, this model
exhibits reasoning-oriented behavior comparable to smaller hosted reasoning
models, while remaining inspectable and self-hostable~\cite{openai2025gptoss}.
The choice emphasized transparency and controllability over raw performance.

For retrieval-augmented generation (RAG) experiments, the embedding model
\texttt{nomic-embed-text} was used to generate vector representations of
documentation content~\cite{nomic2025embed}. This allowed the entire RAG
pipeline to operate locally without reliance on external embedding services,
facilitating rapid iteration and debugging.

%=============================================================================
\section{Serving Architecture Evolution}
\label{sec:LocalServing_Architecture}
%=============================================================================

\subsection{Initial Prototyping with Ollama}

Early experiments used \texttt{Ollama}, a lightweight runtime for executing
open-source LLMs on local hardware~\cite{ollama}. Ollama provided a convenient
HTTP-based interface and minimal configuration overhead, making it suitable
for early-stage prototyping and functional validation.

This setup enabled fast iteration on prompts, tool interfaces, and agent logic.
However, performance quickly became constrained under concurrent requests, and
the serving stack provided limited visibility into GPU utilization and memory
pressure.

\subsection{Transition to vLLM}

To better evaluate realistic serving behavior, the serving stack was migrated
to \texttt{vLLM}, an inference engine designed for high-throughput LLM
serving~\cite{kwon2023efficient}. vLLM introduces two key mechanisms:
\textit{PagedAttention}, which reduces KV-cache fragmentation, and continuous
batching, which maximizes GPU utilization across concurrent requests.

As illustrated in Figure, the vLLM serving architecture consisted of:

\begin{itemize}
    \item containerized vLLM instances deployed via Docker and Docker Compose,
    \item multiple model endpoints exposed on separate ports,
    \item an \texttt{Nginx} reverse proxy for per-model request routing,
    \item secure external access provided through \texttt{Tailscale Funnel}.
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{Figures/model-serving.png}
    \caption{vLLM serving architecture diagram illustrating the main components and their interactions.}
    \label{fig:vllm-serving}
\end{figure}

This setup enabled controlled experiments with concurrency, batching, and
model co-location on a single GPU, while remaining operationally simple.

%=============================================================================
\section{Hardware Experiments and Constraints}
\label{sec:LocalServing_Hardware}
%=============================================================================

Local serving was evaluated across multiple hardware environments.

On a development laptop with an RTX 3060 GPU, vLLM could not run within
practical constraints, and Ollama frequently spilled computation to the CPU
due to memory pressure. This resulted in high latency and unstable performance,
confirming that consumer-grade GPUs are unsuitable for sustained interactive
workloads.

On a dedicated server with an RTX 4090 (24~GB VRAM), vLLM hosted
\texttt{gpt-oss:20b} with acceptable latency under moderate concurrency.
Empirical observation showed that KV-cache allocation consumed several
gigabytes of GPU memory, making KV-cache capacity a first-order constraint on
both context length and parallel request handling~\cite{pope2022efficiently}.

These experiments demonstrated that careful tuning of batching parameters,
worker counts, and KV-cache limits is required to co-locate multiple models on
a single GPU without degrading responsiveness.

%=============================================================================
\section{Evaluation and Outcomes}
\label{sec:LocalServing_Evaluation}
%=============================================================================

Evaluation was primarily qualitative and experience-driven, focusing on
responsiveness, stability under load, and suitability for iterative
development. No formal benchmark suite was used, as the goal was architectural
understanding rather than peak performance measurement.

The experiments showed that local model serving is effective for early
research, debugging, and cost-sensitive prototyping. However, even with vLLM,
local deployments remain constrained by hardware limits and operational
complexity, making them unsuitable as a long-term solution for production
systems requiring low latency, elasticity, and strong reliability guarantees.

These findings directly motivated the transition toward managed serving
platforms and more structured orchestration frameworks in later stages of the
internship.

%=============================================================================
\section{Key Learning Points}
\label{sec:LocalServing_Learnings}
%=============================================================================

Several lessons emerged from this exploration:

\begin{itemize}
    \item Local serving lowers the barrier to experimentation but does not
          remove fundamental performance and scaling constraints.
    \item vLLM provides substantial improvement over lightweight runtimes by
          enabling efficient batching and KV-cache management, making realistic
          concurrency experiments possible.
    \item Serving architecture choices strongly influence downstream system
          design, particularly for interactive and human-in-the-loop agents.
\end{itemize}

Local model serving proved valuable as an exploratory and evaluative tool,
shaping informed decisions about production architecture rather than serving as
a production endpoint itself.